# AI-Code-Evaluation-Normal-RAG-Demo

This project demonstrates **Standard RAG (Normal RAG)** used for evaluating student code by comparing it with instructor code using **similarity search**, **chunking**, and **embeddings** stored inside a **FAISS vector database**.

> âš ï¸ This demo evaluates by **similarity**, not by understanding logic or grading criteria.
> That is why Normal RAG is **not suitable** for real code evaluation.

---

## ğŸ“‚ Project Structure

```
ğŸ“¦ Normal_RAG_demo
 â”£ ğŸ“‚ instructor_code                 # Teacherâ€™s reference solution
 â”ƒ â”£ ğŸ“‚ components
 â”ƒ â”ƒ â”£ ğŸ“œ Footer.jsx                  # Instructor footer component
 â”ƒ â”ƒ â”£ ğŸ“œ Header.jsx                  # Instructor header component
 â”ƒ â”ƒ â”£ ğŸ“œ TodoItem.js                 # Instructor todo item logic
 â”ƒ â”ƒ â”— ğŸ“œ TodoList.js                 # Instructor todo list logic
 â”ƒ â”£ ğŸ“‚ utils
 â”ƒ â”ƒ â”— ğŸ“œ helper.js                   # Instructor helper functions
 â”ƒ â”£ ğŸ“œ App.jsx                       # Instructor main React app
 â”ƒ â”— ğŸ“œ index.js                      # Instructor entry point
 â”£ ğŸ“‚ student_code                    # Student project submission
 â”ƒ â”£ ğŸ“‚ components
 â”ƒ â”ƒ â”£ ğŸ“œ Footer.jsx                  # Student footer component
 â”ƒ â”ƒ â”£ ğŸ“œ Header.jsx                  # Student header component
 â”ƒ â”ƒ â”£ ğŸ“œ TodoItem.js                 # Student todo item logic
 â”ƒ â”ƒ â”— ğŸ“œ TodoList.js                 # Student todo list logic
 â”ƒ â”£ ğŸ“‚ utils
 â”ƒ â”ƒ â”— ğŸ“œ helper.js                   # Student helper functions
 â”ƒ â”£ ğŸ“œ App.jsx                       # Student main React app
 â”ƒ â”— ğŸ“œ index.js                      # Student entry point
 â”£ ğŸ“œ chunk_summary.txt               # Summary of all generated code chunks
 â”£ ğŸ“œ rag_agent.py                    # Step 3: evaluates similarity and generates feedback
 â”£ ğŸ“œ similarity_results.txt          # Output file from Step 2 (top matches)
 â”£ ğŸ“œ step1_chunk_embed.py            # Step 1: chunk + embed instructor code into FAISS
 â”£ ğŸ“œ step2_query.py                  # Step 2: compare student chunks with teacher chunks
 â”— ğŸ“œ student_feedback.txt            # Final scoring and feedback for the student (Step 3)

```

---

# âš™ï¸ 1. Installation Guide

## âœ” Step 1 â€” Create virtual environment

```bash
python -m venv venv
```

## âœ” Step 2 â€” Activate it

### VScode:

```bash
source venv/Scripts/activate
```

### Mac / Linux:

```bash
source venv/bin/activate
```

## âœ” Step 3 â€” Install all dependencies

```bash
pip install -r requirements.txt
```

You do **not** need to install anything manually.
`requirements.txt` contains everything required.

---

# â–¶ï¸ 2. How to Run the Pipeline

### **Step 1 â€” Chunk + Embed instructor & student code**

This cuts the code into small pieces and stores embeddings into FAISS.

```bash
python step1_chunk_embed.py
```

Output includes:

* number of instructor files loaded
* number of student files loaded
* number of chunks produced
* FAISS database saved to folder: `faiss_demo/`

---

### **Step 2 â€” Query: Compare student vs instructor**

This loads FAISS and returns the most similar instructor chunks for each student chunk.

```bash
python step2_query.py
```

Output includes:

* preview of student chunks
* top-2 closest instructor chunks
* similarity comparison

---

### **Step 3 â€” Evaluate using simple RAG agent**

This applies a **dummy evaluator** to generate:

* feedback
* score (based on similarity)

```bash
python rag_agent.py
```

This is NOT a real evaluator â€” just a demo of how a RAG agent would work.

---

# ğŸ§  How Normal RAG Works (Short Summary)

1. **Teacher code** + **student code** â†’ loaded
2. Code is **chunked** (ex: 300 characters per chunk)
3. Each chunk is **embedded** â†’ converted into a vector
4. All vectors stored inside **FAISS**
5. Student chunks are searched against instructor chunks
6. Similarity determines:

   * what feedback is shown
   * what score is produced

Normal RAG does **not** understand code or logic.
It only measures **text similarity**, which is why it is *not suitable* for real grading.

---

# ğŸ“„ Requirements

Example of what your `requirements.txt` should contain:

```
langchain
langchain-community
langchain-text-splitters
faiss-cpu
numpy
```

(You may adjust based on your actual version.)

---

# ğŸ“ For Teachers

This demo shows why **normal RAG is not effective** for educational code evaluation:

* It cannot analyze logic
* It cannot detect errors
* It cannot apply scoring rubrics
* It only compares text similarity

For realistic evaluation, an **Agentic RAG system** is required.

---
